"""Enhanced context generator with multi-file support."""

import json
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, field

from .config import workflow_config
from .jira_connector import PBIData


@dataclass
class ContextFiles:
    """Generated context file paths."""
    requirements: Path
    tests: Path
    implementation: Path
    todo: Path
    test_skeleton: Optional[Path] = None


@dataclass 
class TodoItem:
    """Interactive todo item."""
    id: int
    title: str
    status: str = "pending"  # pending, in-progress, done
    category: str = "general"  # requirement, test, implementation
    
    def to_dict(self):
        return {
            "id": self.id,
            "title": self.title,
            "status": self.status,
            "category": self.category
        }


class EnhancedContextGenerator:
    """Generates multi-file context for Copilot with interactive TODO."""
    
    def __init__(self, output_dir: Optional[str] = None):
        self.output_dir = Path(output_dir or workflow_config.context_dir)
    
    def generate(self, pbi: PBIData, working_dir: Path) -> ContextFiles:
        """
        Generate all context files.
        
        Args:
            pbi: Parsed PBI data
            working_dir: Working directory for the project
            
        Returns:
            ContextFiles with paths to all generated files
        """
        context_dir = working_dir / self.output_dir / pbi.key
        context_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate all context files
        requirements_file = self._generate_requirements(pbi, context_dir)
        tests_file = self._generate_tests(pbi, context_dir)
        implementation_file = self._generate_implementation(pbi, context_dir)
        todo_file = self._generate_todo(pbi, context_dir)
        
        # Generate test skeleton in project
        test_skeleton = self._generate_test_skeleton(pbi, working_dir)
        
        # Create index file
        self._generate_index(pbi, context_dir, test_skeleton)
        
        return ContextFiles(
            requirements=requirements_file,
            tests=tests_file,
            implementation=implementation_file,
            todo=todo_file,
            test_skeleton=test_skeleton
        )
    
    def _generate_requirements(self, pbi: PBIData, context_dir: Path) -> Path:
        """Generate requirements.md with parsed PBI info."""
        file_path = context_dir / "requirements.md"
        
        ac_list = ""
        if pbi.acceptance_criteria:
            ac_list = "\n".join(f"- [ ] {ac}" for ac in pbi.acceptance_criteria)
        else:
            ac_list = "_No acceptance criteria found._"
        
        labels = ", ".join(f"`{label}`" for label in pbi.labels) if pbi.labels else "_None_"
        
        content = f'''# ðŸ“‹ Requirements: {pbi.key}

## Summary
**{pbi.summary}**

| Field | Value |
|-------|-------|
| Jira | [{pbi.key}]({pbi.url}) |
| Type | {pbi.issue_type} |
| Priority | {pbi.priority} |
| Status | {pbi.status} |
| Labels | {labels} |

---

## Description

{pbi.description if pbi.description else "_No description provided._"}

---

## Acceptance Criteria

{ac_list}

---

## Scope Analysis

### In Scope
_Based on the requirements above, list what IS included:_
- 

### Out of Scope
_List what is NOT included in this PBI:_
- 

### Dependencies
_External dependencies or blockers:_
- 

---

## Copilot Prompt

```
@workspace Analyze the requirements in #file:{pbi.key}/requirements.md
- What components/modules are affected?
- What are the edge cases to consider?
- Any potential risks or blockers?
```

---
_Generated by Agentic Workflow_
'''
        file_path.write_text(content, encoding='utf-8')
        return file_path
    
    def _generate_tests(self, pbi: PBIData, context_dir: Path) -> Path:
        """Generate tests.md with TDD guidance."""
        file_path = context_dir / "tests.md"
        
        # Generate test cases from AC
        test_cases = self._generate_test_cases(pbi)
        
        content = f'''# ðŸ§ª Test Plan: {pbi.key}

## TDD Approach

```
ðŸ”´ RED    â†’ Write failing tests first
ðŸŸ¢ GREEN  â†’ Write minimum code to pass
ðŸ”µ BLUE   â†’ Refactor while keeping green
```

---

## Test Cases

{test_cases}

---

## Test Categories

### Unit Tests
_Test individual functions/methods:_
- [ ] Test happy path
- [ ] Test edge cases
- [ ] Test error handling

### Integration Tests
_Test component interactions:_
- [ ] Test API endpoints (if applicable)
- [ ] Test database operations (if applicable)
- [ ] Test external service calls (if applicable)

---

## Test File Location

```
tests/test_{pbi.key.lower().replace('-', '_')}.py
```

---

## Copilot Prompts

### Generate Unit Tests
```
@workspace Based on #file:{pbi.key}/requirements.md and #file:{pbi.key}/tests.md
Generate comprehensive pytest test cases for {pbi.key}.
Include edge cases and error scenarios.
```

### Generate Test Fixtures
```
@workspace Create pytest fixtures for {pbi.key} tests.
Include mock data and setup/teardown helpers.
```

---
_Generated by Agentic Workflow_
'''
        file_path.write_text(content, encoding='utf-8')
        return file_path
    
    def _generate_test_cases(self, pbi: PBIData) -> str:
        """Generate test case suggestions from AC."""
        if not pbi.acceptance_criteria:
            return "| # | Test Case | AC | Priority |\n|---|-----------|----|---------|\n| 1 | _Define test cases_ | - | High |"
        
        rows = ["| # | Test Case | AC Reference | Priority |", "|---|-----------|--------------|----------|"]
        
        for i, ac in enumerate(pbi.acceptance_criteria, 1):
            # Create test name from AC
            test_name = ac[:50] + "..." if len(ac) > 50 else ac
            test_func = self._ac_to_test_name(ac)
            priority = "High" if i <= 3 else "Medium"
            rows.append(f"| {i} | `{test_func}` | {test_name} | {priority} |")
        
        return "\n".join(rows)
    
    def _ac_to_test_name(self, ac: str) -> str:
        """Convert acceptance criteria to test function name."""
        # Clean and convert to snake_case
        clean = ''.join(c if c.isalnum() or c == ' ' else '' for c in ac.lower())
        words = clean.split()[:5]
        return f"test_{'_'.join(words)}"
    
    def _generate_implementation(self, pbi: PBIData, context_dir: Path) -> Path:
        """Generate implementation.md with coding guidance."""
        file_path = context_dir / "implementation.md"
        
        content = f'''# ðŸ”¨ Implementation: {pbi.key}

## Approach

_Document the implementation approach here:_

### Architecture Decisions
- 

### Files to Create/Modify
| File | Action | Description |
|------|--------|-------------|
| | Create/Modify | |

### Dependencies to Add
```
# Add to requirements.txt or package.json
```

---

## Implementation Checklist

- [ ] Review requirements in `requirements.md`
- [ ] All tests written (from `tests.md`)
- [ ] Tests passing (green)
- [ ] Code reviewed and refactored
- [ ] Documentation updated
- [ ] No linting errors

---

## Code Structure

```
# Suggested structure for {pbi.key}
src/
â”œâ”€â”€ [module]/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ [feature].py      # Main implementation
â”‚   â””â”€â”€ [feature]_utils.py # Helper functions
tests/
â””â”€â”€ test_{pbi.key.lower().replace('-', '_')}.py
```

---

## Implementation Notes

_Add notes as you implement:_

### Decisions Made
- 

### Issues Encountered  
- 

### Future Improvements
- 

---

## Copilot Prompts

### Implement Feature
```
@workspace Based on #file:{pbi.key}/requirements.md and #file:{pbi.key}/tests.md
Implement the feature for {pbi.key}.
Make sure all tests pass.
```

### Refactor Code
```
@workspace Review and refactor the implementation for {pbi.key}.
Focus on:
- Code clarity
- Performance
- Error handling
```

---
_Generated by Agentic Workflow_
'''
        file_path.write_text(content, encoding='utf-8')
        return file_path
    
    def _generate_todo(self, pbi: PBIData, context_dir: Path) -> Path:
        """Generate interactive TODO checklist."""
        file_path = context_dir / "todo.json"
        
        todos = []
        todo_id = 1
        
        # Requirements todos
        todos.append(TodoItem(todo_id, "Review requirements", "pending", "requirement").to_dict())
        todo_id += 1
        todos.append(TodoItem(todo_id, "Analyze scope & dependencies", "pending", "requirement").to_dict())
        todo_id += 1
        
        # Test todos from AC
        for ac in pbi.acceptance_criteria[:5]:
            short_ac = ac[:40] + "..." if len(ac) > 40 else ac
            todos.append(TodoItem(todo_id, f"Test: {short_ac}", "pending", "test").to_dict())
            todo_id += 1
        
        # Implementation todos
        todos.append(TodoItem(todo_id, "Implement core feature", "pending", "implementation").to_dict())
        todo_id += 1
        todos.append(TodoItem(todo_id, "Handle edge cases", "pending", "implementation").to_dict())
        todo_id += 1
        todos.append(TodoItem(todo_id, "Add error handling", "pending", "implementation").to_dict())
        todo_id += 1
        todos.append(TodoItem(todo_id, "Refactor & cleanup", "pending", "implementation").to_dict())
        todo_id += 1
        todos.append(TodoItem(todo_id, "Update documentation", "pending", "implementation").to_dict())
        todo_id += 1
        
        todo_data = {
            "pbi_key": pbi.key,
            "summary": pbi.summary,
            "created_at": "",  # Will be set by todo manager
            "todos": todos
        }
        
        file_path.write_text(json.dumps(todo_data, indent=2), encoding='utf-8')
        
        # Also generate markdown view
        self._generate_todo_markdown(pbi, context_dir, todos)
        
        return file_path
    
    def _generate_todo_markdown(self, pbi: PBIData, context_dir: Path, todos: list):
        """Generate human-readable TODO markdown."""
        file_path = context_dir / "todo.md"
        
        # Group by category
        requirements = [t for t in todos if t["category"] == "requirement"]
        tests = [t for t in todos if t["category"] == "test"]
        implementation = [t for t in todos if t["category"] == "implementation"]
        
        def render_todos(items):
            return "\n".join(f"- [ ] {t['title']}" for t in items)
        
        content = f'''# âœ… TODO: {pbi.key}

> **{pbi.summary}**
> 
> Use `agentic todo {pbi.key}` to manage interactively

---

## ðŸ“‹ Requirements
{render_todos(requirements)}

## ðŸ§ª Tests  
{render_todos(tests)}

## ðŸ”¨ Implementation
{render_todos(implementation)}

---

## Progress

```
Total: {len(todos)} tasks
Done:  0 / {len(todos)} (0%)
```

---
_Update todo.json to track progress_
'''
        file_path.write_text(content, encoding='utf-8')
    
    def _generate_test_skeleton(self, pbi: PBIData, working_dir: Path) -> Optional[Path]:
        """Generate test skeleton file in project tests folder."""
        tests_dir = working_dir / "tests"
        tests_dir.mkdir(exist_ok=True)
        
        # Create __init__.py if not exists
        init_file = tests_dir / "__init__.py"
        if not init_file.exists():
            init_file.write_text("", encoding='utf-8')
        
        test_file = tests_dir / f"test_{pbi.key.lower().replace('-', '_')}.py"
        
        # Generate test functions from AC
        test_functions = self._generate_test_functions(pbi)
        
        content = f'''"""
Tests for {pbi.key}: {pbi.summary}

Generated by Agentic Workflow - TDD approach
Run: pytest {test_file.name} -v
"""

import pytest


# ============================================
# Fixtures
# ============================================

@pytest.fixture
def sample_data():
    """Sample test data fixture."""
    # TODO: Add test data
    return {{}}


# ============================================
# Tests for {pbi.key}
# ============================================

{test_functions}


# ============================================
# Edge Cases
# ============================================

class TestEdgeCases:
    """Edge case tests for {pbi.key}."""
    
    def test_empty_input(self):
        """Test behavior with empty input."""
        # TODO: Implement
        pytest.skip("Not implemented yet")
    
    def test_invalid_input(self):
        """Test behavior with invalid input."""
        # TODO: Implement
        pytest.skip("Not implemented yet")
    
    def test_boundary_conditions(self):
        """Test boundary conditions."""
        # TODO: Implement
        pytest.skip("Not implemented yet")


# ============================================
# Error Handling
# ============================================

class TestErrorHandling:
    """Error handling tests for {pbi.key}."""
    
    def test_raises_on_error(self):
        """Test that appropriate errors are raised."""
        # TODO: Implement
        pytest.skip("Not implemented yet")
'''
        test_file.write_text(content, encoding='utf-8')
        return test_file
    
    def _generate_test_functions(self, pbi: PBIData) -> str:
        """Generate test function stubs from AC."""
        if not pbi.acceptance_criteria:
            return '''def test_placeholder():
    """Placeholder test - add acceptance criteria to generate tests."""
    pytest.skip("Add acceptance criteria to Jira PBI")
'''
        
        functions = []
        for i, ac in enumerate(pbi.acceptance_criteria, 1):
            func_name = self._ac_to_test_name(ac)
            short_ac = ac[:70] + "..." if len(ac) > 70 else ac
            
            functions.append(f'''def {func_name}():
    """
    AC {i}: {short_ac}
    """
    # TODO: Implement test
    # Arrange
    
    # Act
    
    # Assert
    pytest.skip("Not implemented yet")
''')
        
        return "\n\n".join(functions)
    
    def _generate_index(self, pbi: PBIData, context_dir: Path, test_skeleton: Optional[Path]):
        """Generate index.md linking all context files."""
        file_path = context_dir / "index.md"
        
        test_skeleton_link = ""
        if test_skeleton:
            test_skeleton_link = f"- ðŸ“„ [Test Skeleton](../../{test_skeleton.relative_to(test_skeleton.parent.parent)})"
        
        content = f'''# ðŸš€ {pbi.key}: {pbi.summary}

> Quick navigation for Copilot-assisted development

---

## Context Files

- ðŸ“‹ [Requirements](requirements.md) - PBI details & acceptance criteria
- ðŸ§ª [Tests](tests.md) - TDD test plan & cases
- ðŸ”¨ [Implementation](implementation.md) - Coding guidance & structure
- âœ… [TODO](todo.md) - Interactive checklist
{test_skeleton_link}

---

## Workflow

```
1. Read requirements.md     â†’ Understand scope
2. Review tests.md          â†’ Plan TDD approach  
3. Write tests              â†’ ðŸ”´ RED
4. Implement code           â†’ ðŸŸ¢ GREEN
5. Refactor                 â†’ ðŸ”µ BLUE
6. Check todo.md            â†’ Track progress
```

---

## Quick Copilot Prompts

### Full Context Analysis
```
@workspace Analyze all files in #file:{pbi.key}/
Create implementation plan for {pbi.key}
```

### Start TDD
```
@workspace Using #file:{pbi.key}/tests.md
Complete the test implementations in the test skeleton file
```

---

**Jira:** [{pbi.key}]({pbi.url})

_Generated by Agentic Workflow_
'''
        file_path.write_text(content, encoding='utf-8')
        return file_path


# Singleton instance
enhanced_context_generator = EnhancedContextGenerator()
