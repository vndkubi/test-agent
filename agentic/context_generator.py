"""Context generator for GitHub Copilot prompts."""

from pathlib import Path
from typing import Optional

from .config import workflow_config
from .jira_connector import PBIData


class ContextGenerator:
    """Generates structured context files for Copilot."""
    
    def __init__(self, output_dir: Optional[str] = None):
        self.output_dir = Path(output_dir or workflow_config.context_dir)
    
    def generate(self, pbi: PBIData, working_dir: Path) -> Path:
        """Generate context markdown file for Copilot."""
        context_dir = working_dir / self.output_dir
        context_dir.mkdir(parents=True, exist_ok=True)
        
        context_file = context_dir / workflow_config.context_file
        content = self._build_context(pbi)
        context_file.write_text(content, encoding='utf-8')
        
        return context_file
    
    def _build_context(self, pbi: PBIData) -> str:
        """Build the context markdown content."""
        
        ac_list = ""
        if pbi.acceptance_criteria:
            ac_list = "\n".join(f"- [ ] {ac}" for ac in pbi.acceptance_criteria)
        else:
            ac_list = "_No acceptance criteria found. Please add manually._"
        
        labels = ", ".join(f"`{label}`" for label in pbi.labels) if pbi.labels else "_None_"
        
        return f'''# {pbi.key}: {pbi.summary}

> **Jira Link:** [{pbi.key}]({pbi.url})  
> **Type:** {pbi.issue_type} | **Priority:** {pbi.priority} | **Status:** {pbi.status}  
> **Labels:** {labels}

---

## ðŸ“‹ Description

{pbi.description if pbi.description else "_No description provided._"}

---

## âœ… Acceptance Criteria

{ac_list}

---

## ðŸ§ª TDD Approach

### Step 1: Write Tests First (Red)
Based on the acceptance criteria above, create test cases that will fail initially.

```
Suggested test file: tests/test_{pbi.key.lower().replace('-', '_')}.py
```

**Test cases to implement:**
{self._generate_test_suggestions(pbi)}

### Step 2: Implement Code (Green)
Write the minimum code necessary to make all tests pass.

### Step 3: Refactor (Blue)
Clean up the code while keeping tests green.

---

## ðŸ“ Implementation Notes

_Add your implementation notes here as you work..._

---

## ðŸ”— Copilot Prompts

### Analyze Requirements
```
@workspace Analyze this PBI and suggest the implementation approach:
- Key: {pbi.key}
- Summary: {pbi.summary}
- What files need to be created/modified?
- What dependencies are needed?
```

### Generate Tests
```
@workspace Generate pytest test cases for {pbi.key} based on these acceptance criteria:
{chr(10).join(f"- {ac}" for ac in pbi.acceptance_criteria[:5]) if pbi.acceptance_criteria else "- (add criteria)"}
```

### Implement Feature
```
@workspace Implement the feature for {pbi.key} to make all tests pass.
Follow TDD - the tests are already written.
```

---

_Generated by Agentic Workflow Tool_
'''
    
    def _generate_test_suggestions(self, pbi: PBIData) -> str:
        """Generate suggested test cases from acceptance criteria."""
        if not pbi.acceptance_criteria:
            return "- _Define test cases based on requirements_"
        
        suggestions = []
        for i, ac in enumerate(pbi.acceptance_criteria[:10], 1):
            test_name = ac.lower()
            test_name = ''.join(c if c.isalnum() or c == ' ' else '' for c in test_name)
            test_name = '_'.join(test_name.split()[:6])
            suggestions.append(f"- `test_{test_name}` â†’ {ac[:80]}{'...' if len(ac) > 80 else ''}")
        
        return "\n".join(suggestions)


context_generator = ContextGenerator()
